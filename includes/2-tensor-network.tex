\chapter{张量网络方法介绍}
\label{chap:tensor-network}

\section{基本概念}

\emph{张量网络} (tensor network)\cite{orus2014practical,bridgeman2017hand,biamonte2017tensor,orus2019tensor,ran2020tensor,evenbly2022practical} 为凝聚态物理、量子信息、机器学习等领域提供了一套统一的描述框架。顾名思义，张量网络就是根据一定规则连接起来张量单元。这里的\emph{张量} (tensor) 可以简单理解为多维数组，即标量（0 维张量）、向量（1 维张量）、矩阵（2 维张量）的推广。

如图~\ref{fig:tensors} 所示，张量网络常利用图形方式来描述，其中圆圈（也可用其他图形）表示张量，延伸出来的腿表示张量的指标。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.6\textwidth]{images/temp/tensors.pdf}
  \caption[张量单元]{三种张量单元，分别为向量、矩阵和三阶张量。}
  \label{fig:tensors}
\end{figure}

\subsection{基本张量运算}

常用的张量运算包括张量积、缩并、变形等。

\emph{张量积} (tensor product) 其实就是把若干个张量并排放置，并保持指标不变：
\begin{equation}
  (A \otimes B)_{i_1,\dots,i_r,j_1,\dots,j_s} \coloneq A_{i_1,\dots,i_r} B_{j_1,\dots,j_s}.
\end{equation}
图形描述为
\begin{equation}
  [[TODO:]]
\end{equation}

张量的\emph{缩并} (contraction) 是向量内积、矩阵乘法的推广，即对两个张量的某些指标进行求和，得到一个新的张量：
\begin{equation}
  C_{abc} = \sum_k A_{abk} B_{kc} \eqcolon A_{abk} B_{kc}
\end{equation}
这里根据 Einstein 求和约定省略了求和号。张量缩并的图形描述为
\begin{equation}
  \mbox{\includegraphics[width=0.75\textwidth]{images/temp/contraction.pdf}}
\end{equation}
即把需要缩并的腿（指标）连接起来。

同一个张量的指标也可以缩并，这样就得到了\emph{迹} (trace) 或\emph{偏迹} (partial trace)：
\begin{equation}
    (\tr_{x,y} A)_{i_1,\dots,i_{x-1},i_{x+1},\dots,i_{y-1},u_{y+1},\dots,i_r}
  = A_{i_1,\dots,i_{x-1},k,i_{x+1},\dots,i_{y-1},k,i_{y+1},\dots,i_r}.
\end{equation}
其图形描述与缩并类似：
\begin{equation}
  [[TODO:]]
\end{equation}
利用图形语言很容易验证 $\tr(AB)=\tr(BA)$：
\begin{equation}
  [[TODO:]]
\end{equation}

张量的\emph{变形} (reshape) 相当于指标的重新组合，例如
\begin{equation}
  \mbox{\includegraphics[width=0.5\textwidth]{images/temp/reshape.pdf}}
\end{equation}
在数值计算中，把一般形状的张量变形为矩阵，往往可以利用为矩阵优化的算法来加速计算。

\subsection{张量分解}

张量的\emph{分解} (decomposition) 可以理解为缩并的逆运算，最常用的是\emph{奇异值分解} (singular value decomposition, SVD)，即
\begin{equation}
  M_{ij} = \sum_{k=1}^{\min(m,n)} U_{ik} \Lambda_{kk} V_{kj} = \sum_k U_{ik} \Lambda_{kk} \bigl( V^\dagger \bigr)_{jk},
\end{equation}
其中 $M$ 为 $m\times n$ 矩阵，$\Lambda$ 为对角矩阵（如果行数、列数不等，则相应补零），$U$ 和 $V$ 为幺正矩阵，$V^\dagger$ 表示共轭转置。利用张量的变形，可以很容易地将 SVD 推广为一般形状的张量，用图形描述为
\begin{equation}
  [[TODO:]]
\end{equation}

奇异值分解可以用来获得张量的近似表示。将奇异值（$\Lambda$ 矩阵的对角元）按大小排列后，只保留前 $r$ 个值，就得到了秩为 $r$ 的近似张量：
\begin{equation}
  M_{ij} \approx \sum_{k=1}^r U_{ik} \Lambda_{kk} V_{kj}, \quad r < \min(m,n).
\end{equation}
并且可以证明，这种近似表示是所有秩为 $r$ 的张量中最优的。

对于一个一般的张量，原则上我们总可以把它改写为若干个小张量的缩并形式，并保证自由度数目不变。这种改写并不能降低计算复杂度，但我们可以利用奇异值分解把这些小张量替换为对应的近似表示，这样就可以大幅减少总自由度的数目。

在量子力学的语境中，奇异值分解也可表述为 \emph{Schmidt 分解} (Schmidt decomposition)：对于 Hilbert 空间 $\mathcal{H}^{\text{L}}\otimes\mathcal{H}^{\text{R}}$，其中的任意量子态 $\ket{\Psi}$ 均可被分解为
\begin{equation}
  \ket{\Psi} = \sum_\alpha \lambda_\alpha \ket{\Phi^{\text{L}}_\alpha} \otimes \ket{\Phi^{\text{R}}_\alpha},
  \label{eq:schmidt-decomposition}
\end{equation}
其中 $\lambda_\alpha$ 称为 \emph{Schmidt 系数} (Schmidt coefficients)，而 $\ket{\Phi^{\text{L}}_\alpha}$、$\ket{\Phi^{\text{R}}_\alpha}$ 分别是 $\mathcal{H}^{\text{L}}$、$\mathcal{H}^{\text{R}}$ 中的单位正交基，称为 \emph{Schmidt 向量} (Schmidt vectors)。可以发现 Schmidt 系数和奇异值是等价的，而 Schmidt 向量和幺正矩阵 $U$、$V$ 也是等价的。

\section{矩阵乘积态算法}

接下来介绍几种具体的张量网络及其算法。本文中，我们把这些算法分为两类：一类算法以矩阵乘积态为代表，主要用来处理一维量子多体系统；另一类算法则以张量重整化群为代表，顾名思义，主要用在二维网络的重整化（粗粒近似）操作中。

\subsection{波函数的构造}
\label{subsec:mps-construction}

% \citet{cirac2021matrix}

如图~\ref{fig:mps} 所示，\emph{矩阵乘积态} (matrix product state, MPS)\cite{perez2007matrix,verstraete2008matrix,orus2014practical} 由一系列张量单元首尾相连而成，每个张量单元包含三个指标，没有缩并的称为\emph{物理指标} (physical index)，另外两个则称为\emph{虚拟指标} (virtual index) 或\emph{辅助指标} (auxiliary index)。根据具体问题的需要，MPS 可取开放或周期性边界条件。当所研究的系统具有平移对称性时，可以将所有张量单元取为相同值，此时有
\begin{equation}
  % TODO: 周期性边界条件
  \Psi_{i_1 i_2 \cdots i_n} = A^{j_1 j_2}_{i_1} A^{j_2 j_3}_{i_2} \cdots A^{j_n j_1}_{i_n}.
\end{equation}
这样的张量网络称为\emph{无限 MPS} (infinite MPS, iMPS)。我们之后的讨论主要针对 iMPS。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.6\textwidth]{images/temp/mps.png}
  \caption[矩阵乘积态的示意图]{矩阵乘积态的示意图。图 (a)、(b) 分别对应开放和周期性边界条件。}
  \label{fig:mps}
\end{figure}

考虑一个量子态 $\ket{\Psi}$，我们对它作用一次 SVD，便可以获得由两个张量组成的 MPS，此时连接维数 $\chi$ 等于奇异值的数量。这一过程可以不断重复，以构造出任意长度的 MPS。在做 SVD 时，如果不对奇异值进行截断，可以发现 $\chi$ 会随着分解的次数（也即张量单元的数量）$N$ 指数级增长。然而对于一维\emph{有能隙} (gapped) Hamilton 量的基态以及低能激发态，$\chi$ 可以取为常数；对于\emph{无能隙} (gapless) 的临界系统，则会以多项式形式发散。

% TODO: \emph{面积定律} (area law)

由上述方法构造出的 MPS 并不是唯一的，而是存在所谓\emph{规范自由度} (gauge freedom)\cite{bridgeman2017hand}。如图~\ref{fig:mps-gauge-freedom} 所示，两个张量单元之间总可以插入单位矩阵 $I=XX^{-1}$，并把 $X$ 和 $X^{-1}$ 分配到两边。

\begin{figure}[htb]
  \centering
  % \includegraphics[width=0.6\textwidth]{images/temp/mps.png}
  % \caption[]{}
  \caption{规范自由度}
  \label{fig:mps-gauge-freedom}
\end{figure}

% TODO: 密度矩阵
为了方便计算，我们一般会把 MPS 取为\emph{正则形式} (canonical form)\cite{orus2008infinite,schollwock2011density,orus2014practical}，它要求辅助指标由 Schmidt 分解式~\eqref{eq:schmidt-decomposition} 决定。张量单元 $A$ 会被拆分成 $\Gamma$ 和 $\lambda$ 两部分，分别对应 Schmidt 向量与系数。定义左右矩阵
\begin{equation}
  \begin{aligned}
       R_{(\alpha\alpha'), \, (\beta\beta')}
    &= \sum_{i=1}^d \left( \Gamma^i_{\alpha\beta} \lambda_\beta \right) \left( \Gamma^i_{\alpha'\beta'} \lambda_{\beta'} \right)^*, \\
       L_{(\alpha\alpha'), \, (\beta\beta')}
    &= \sum_{i=1}^d \left( \lambda_\alpha \Gamma^i_{\alpha\beta} \right) \left( \lambda_{\alpha'} \Gamma^i_{\alpha'\beta'} \right)^*,
  \end{aligned}
\end{equation}
此时正则形式要求 $R$、$L$ 的特征向量均为单位矩阵（在张量变形的意义下），且对应的主特征值\footnote{即绝对值最大的特征值。它对应的特征向量也称主特征向量。} $\eta$ 相同。将一般的 $\Gamma$ 和 $\lambda$（例如随机初始化的）进行正则化的方法为：

\begin{enumerate}
  \item 计算 $R$、$L$ 的主特征向量，并变形为矩阵 $V_R$、$V_L$\footnote{在实际计算中，$V_R$、$V_L$ 可能会带有冗余的相位，这会影响接下来的矩阵分解操作。此相位可通过规定 $V_R$、$V_L$ 的迹为实数来移除。}，对应的特征值均为 $\eta$。将 $V_R$、$V_L$ 进一步分解为 $X$、$Y$，使其满足
    \begin{equation}
      V_R = X X^\dagger, \quad V_L = Y^\dagger Y.
    \end{equation}
    这里可以用特征值分解，也可以用 Cholesky 分解。

  \item 利用规范自由度在 $\lambda$ 两侧分别插入 $I=(Y^\trans)^{-1}Y^\trans$ 和 $I=XX^{-1}$，并对得到的 $Y^\trans\lambda X$ 进行奇异值分解：
    \begin{equation}
      Y^\trans \lambda X = U \lambda V^\dagger.
    \end{equation}

  \item 将新得到的张量重排为 $\Gamma'$
    \begin{equation}
      \Gamma' = V^\dagger X^{-1} \Gamma \left( Y^\trans \right)^{-1} U.
    \end{equation}
\end{enumerate}

可以证明通过以上步骤得到的 $\Gamma'$ 和 $\lambda'$ 的确是 iMPS 的正则形式。

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{images/temp/canonical-form.png}
  % \caption[]{}
  \caption{正则形式}
  \label{fig:mps-canonical-form}
\end{figure}

\subsection{基态的确定}

Hamilton 量的基态 $\Psi_0$ 可以通过变分法求得：
\begin{equation}
  \ket{\Psi_0} = \argmin_{\ket{\Psi}} \frac{\langle\Psi|H|\Psi\rangle}{\langle\Psi|\Psi\rangle}.
\end{equation}
即使 $\Psi$ 已经表示成了 MPS 的形式，一般来说对 $\Psi$ 整体进行优化也是无法进行的。

DMRG\cite{white1992density,white1993density,schollwock2005density,mcculloch2007density,schollwock2011density}

iDMRG\cite{mcculloch2008infinite}

\subsection{时间演化}

下面我们介绍\emph{无限时间演化块消减} (infinite time-evolving block decimation, iTEBD)\cite{vidal2007classical,orus2008infinite} 算法，它主要用来处理波函数的时间演化
\begin{equation}
  \ket{\Psi_t} = \ee^{-\ii Ht} \ket{\Psi_0}
\end{equation}
或虚时演化
\begin{equation}
  \ket{\Psi_\tau} = \ee^{-H\tau} \ket{\Psi_0},
\end{equation}
其核心在于通过 Suzuki--Trotter 分解\cite{sornborger1999higher}
\begin{equation}
  \ee^{-\tau(A+B)} = \ee^{-\tau A} \ee^{-\tau B} + \mathcal{O}(\tau^2)
\end{equation}
将演化算符表示成 MPO 的形式，这样就可以很方便地与 MPS 形式的波函数进行缩并。

对于时间演化或虚时演化算符而言，由于经过 Suzuki--Trotter 分解后它们都很接近幺正算符，不会破坏 iMPS 的正则形式。而一般的算符并不具备这一性质，所以需要额外进行正则化操作。一般的 iTEBD 算法如下：

\begin{enumerate}
  \item 取随机的 iMPS $\{\Gamma,\lambda\}$，并按照 \ref{subsec:mps-construction} 小节中介绍的方法将其正则化。

  \item 把 $\{\Gamma,\lambda\}$ 与 MPO 的张量单元进行缩并：
    \begin{equation}
      \tilde{\Gamma}_{j\tilde{\alpha}\tilde{\beta}} = \sum_{i=1}^d \Gamma_{i\alpha\beta} O_{ij\mu\nu}, \quad
      \tilde{\lambda}_{\tilde{\beta}} = \lambda_\beta.
    \end{equation}
    这里指标 $\tilde{\alpha}=(\alpha,\mu)$、$\tilde{\beta}=(\beta,\nu)$，因此得到的 iMPS 对应连接维数 $\tilde{\chi}=\kappa\chi$。

  \item 对 $\{\tilde{\Gamma},\tilde{\lambda}\}$ 进行正则化，得到 $\{\tilde{\Gamma}',\tilde{\lambda}'\}$。

  \item 利用奇异值分解对 $\{\tilde{\Gamma}',\tilde{\lambda}'\}$ 进行截断，即只保留前 $\chi$ 个奇异值，使得连接维数保持在 $\chi$。

  \item 重复步骤 2--4，直到 iMPS 收敛。
\end{enumerate}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.75\textwidth]{images/temp/itebd-evolution.png}
  % \caption[]{}
  \caption{iTEBD 算法}
  \label{fig:itebd-evolution}
\end{figure}

\subsection{配分函数的计算}
\label{subsec:partition-function}

一个不同于时间（虚时）演化的例子，是把二维经典格点模型的配分函数视为 iMPS 在转移矩阵作用下的演化：
\begin{equation}
  Z(\beta) = \lim_{p,q\to\infty} \omega^{pq},
\end{equation}
其中 $\omega$ 是 $W$ 矩阵的主特征值：
\begin{equation}
  \includegraphics[width=0.75\textwidth]{images/temp/itebd-w-matrices.png}
\end{equation}

此时我们还可以计算单点函数和两点（关联）函数的期望值：
\begin{equation}
  \begin{aligned}
    \langle f(\sigma^{\bm{r}}) \rangle
      &= \frac{1}{Z(\beta)} \sum_{\{\sigma\}} f(\sigma^{\bm{r}}) \, \ee^{-\beta H(\{\sigma\})}, \\
    \langle f(\sigma^{\bm{r}}) g(\sigma^{\bm{r}'}) \rangle
      &= \frac{1}{Z(\beta)} \sum_{\{\sigma\}} f(\sigma^{\bm{r}}) g(\sigma^{\bm{r}'}) \, \ee^{-\beta H(\{\sigma\})}.
  \end{aligned}
\end{equation}
主要思路是将原配分函数中的张量单元 $A$ 替换为包含 $f(s)$ 或 $g(s)$ 的 $B$ 或 $B'$，并用同样的方法进行缩并。如图~\ref{fig:expectation-value} 所示，由于 iMPS 已被正则化，我们只需处理一个较小的张量网络。

\begin{figure}[htb]
  \centering
  \includegraphics[height=6.5cm]{images/temp/itebd-one-point-function.png} \quad
  \includegraphics[height=6.5cm]{images/temp/itebd-two-point-function.png}
  \caption{单点函数和两点（关联）函数的计算}
  \label{fig:expectation-value}
\end{figure}

\subsection{推广}

PEPS

MERA, entanglement renormalization\cite{vidal2007entanglement,evenbly2009algorithms,konig2009exact,evenbly2014algorithms,evenbly2015tensor2}

\section{重整化算法}

接下来我们主要考察二维张量网络。与 \ref{subsec:partition-function} 小节相同，我们关注的一个核心问题仍然是格点模型配分函数的计算。类似于 Kadanoff 的实空间重整化群\cite{pathria2011statistical}，张量重整化算法的主要思路是对张量网络进行粗粒近似，直到得到一个不动点张量。

\subsection{张量重整化群}

最基本的一种重整化算法称为\emph{张量重整化群} (tensor renormalization group, TRG)\cite{levin2007tensor}。其步骤为：

\begin{enumerate}
  \item 利用 SVD 对原始的张量单元 $A^{(0)}$ 进行分解：
    \begin{center}
      \includegraphics[width=0.75\linewidth]{images/temp/trg-factorizing.png} \\
      \includegraphics[width=\linewidth]{images/temp/trg-factorizing-svd.png}
    \end{center}
    在做 SVD 时需要对奇异值进行截断，即只保留 $\chi$ 个最大奇异值，这样可以保证 TRG 的计算开销始终在可控范围内。

  \item 把得到的三角形张量缩并为新的张量单元 $A^{(1)}$：
    \begin{center}
      \includegraphics[width=0.4\linewidth]{images/temp/trg-group.png}
    \end{center}
    此时的张量网络相当于旋转了 $45^\circ$，而总的张量数目减少了一半。

  \item 重复以上步骤，直至收敛到不动点张量。

  \item 对最终得到的不动点张量进行求迹操作即可得到配分函数 $Z$：
    \begin{equation}
      Z = \sum_{i,j} A^{(N)}_{ijij} = \raisebox{-2em}{\includegraphics[width=2.5cm]{images/temp/trg-double-trace.png}}
    \end{equation}
\end{enumerate}

在实际计算中直接对 $A^{(n)}$ 进行缩并会很快使数值溢出。一种常用的技术是在每一步中对 $A^{(n)}$ 进行归一化，即令其 Frobenius 范数
\begin{equation}
  \bigl| A^{(n)} \bigr|_{\mathrm{F}} = \left( \sum_{i,j,k,l} A^{(n)}_{ijkl} \right)^{1/2} = 1,
\end{equation}
并记录每步所得的范数。最终的配分函数就相当于不动点张量 $A^{(N)}$ 与这些范数的乘积。

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{images/temp/trg-ising.png}
  \caption{利用 TRG 算法计算二维 Ising 模型的配分函数、能量及热容。可以发现在临界点处 TRG 的计算结果与精确值有一定差异。}
  \label{fig:trg-ising}
\end{figure}

\subsection{张量网络重整化}

TRG 算法对于临界的格点模型效果并不好。这主要是由于临界系统中存在长程关联，纠缠熵
\begin{equation}
  S = -\tr(\rho\log\rho)
\end{equation}
会随着系统尺寸以对数级增长，然而通过 SVD 给出的张量分解并不能很好地保留这些信息。

对于 TRG 的一种改进算法称为\emph{张量网络重整化} (tensor network renormalization, TNR)\cite{evenbly2015tensor1,evenbly2017algorithms}，它在粗粒近似的过程中引入了一组幺正变换
\begin{equation}
  u \colon \mathbb{V} \otimes \mathbb{V} \to \mathbb{V} \otimes \mathbb{V}, \quad
  u u^\dagger = I^{\otimes2}
\end{equation}
和投影算符
\begin{equation}
  v \colon \mathbb{V} \to \mathbb{V} \otimes \mathbb{V}, \quad
  v^\dagger v = I,
\end{equation}
其中 $\mathbb{V}=\mathbb{C}^\chi$ 是 $\chi$ 维复向量空间。这里的 $u$ 和 $v$ 分别称为\emph{解纠缠子} (disentangler) 和\emph{等距子} (isometry)，它们的具体取值可以通过最小化截断误差
\begin{equation}
  \delta = \raisebox{-2em}{\includegraphics[width=6cm]{images/temp/trg-truncation-error.png}}
\end{equation}
来获得。通过角双线 (corner double line, CDL) 张量的方法可以证明\cite{evenbly2015tensor1}，TNR 算法中的 $u$ 和 $v$ 可以消除短程纠缠的影响，使得最终获得的不动点张量的确是标度不变的。

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.6\textwidth]{images/temp/tnr.png}
  \caption{TNR 算法}
  \label{fig:tnr}
\end{figure}

除了 TNR 之外，还有其他一些工作试图改进原始的 TRG 算法，如引入过滤操作以消除短程纠缠影响的\emph{张量纠缠过滤重整化} (tensor entanglement-filtering renormalization, TEFR) 算法\cite{gu2009tensor1}、基于高阶奇异值分解的\emph{高阶 TRG} (higher order TRG, HOTRG) 算法\cite{xie2012coarse}以及通过将小张量组合成环路并加以优化的\emph{环路 TNR} (loop TNR) 算法\cite{yang2017loop}等。这些方法相比 TRG 和 TNR 在精度与计算效率上各有优劣，需要根据具体问题加以权衡。

% CTMRG\cite{nishino1996corner,orus2012exploring}
% Differentiable\cite{liao2019differentiable,geng2022differentiable}

\section{具体实现}

本文后续介绍的算法主要使用 Python 语言实现，其中的张量运算则利用 NumPy\cite{harris2020array}、SciPy\cite{virtanen2020scipy} 编写。它们提供了高效的张量缩并、变形以及 SVD、特征值求解等算法，并且还能通过线性算符 (linear operator) 的方法处理较大规模的稀疏矩阵与张量。此外，在硬件支持的情况下，还可以借助 GPU 甚至 TPU\cite{ganahl2023density} 进一步加速计算过程。

近年来，人们使用多种语言编写了各类张量网络程序包，例如基于 MATLAB 的 NCON\cite{pfeifer2014ncon}，基于 Python 的 TeNPy\cite{hauschild2018efficient}、TensorNetwork\cite{roberts2019tensornetwork} 以及基于 Julia 的 TensorOperations.jl\cite{jutho2023tensoroperations} 等。特别值得注意的是基于 C++ 和 Julia 实现的 ITensor\cite{fishman2022itensor} 包，它能够根据指标本身的性质自动完成张量缩并，无需手动选择求和指标。文献 \parencite{psarras2021landscape} 对这些程序包的功能和特点进行了总结和比较。

在处理较大的系统时，张量缩并往往会成为计算瓶颈。为此人们提出了一系列优化方案\cite{pfeifer2014faster,evenbly2014improving}，以寻找最优的缩并路径。NCON、opt\_einsum\cite{daniel2018opteinsum} 等程序包中均给出了实现。

\section{本章小结}
